{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "# adbe = yfinance.download(['ADBE','AAPL'], start='1992-01-01')\n",
    "# Get the list of all companies listed in NYSE\n",
    "nyse_companies = pd.read_csv('https://pkgstore.datahub.io/core/nyse-other-listings/nyse-listed_csv/data/nyse-listed_csv.csv')\n",
    "symbols = nyse_companies['ACT Symbol'].tolist()\n",
    "\n",
    "# Download data for each company\n",
    "# data = yf.download(symbols, start='1992-01-01')\n",
    "\n",
    "# Print the downloaded data\n",
    "# print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'symbols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msymbols\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'symbols' is not defined"
     ]
    }
   ],
   "source": [
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Get the list of NYSE tickers\n",
    "nyse_tickers = pd.read_csv('listed.csv')['ACT Symbol'].tolist()\n",
    "\n",
    "# Create an empty DataFrame to store the data\n",
    "# stock_data = pd.DataFrame()\n",
    "\n",
    "# # Loop through each ticker and fetch the historical data\n",
    "# for ticker in nyse_tickers:\n",
    "#     try:\n",
    "#         data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-31\")\n",
    "#         data['Ticker'] = ticker\n",
    "#         stock_data = stock_data.append(data)\n",
    "#         print(f\"Fetched data for {ticker}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Could not fetch data for {ticker}: {e}\")\n",
    "\n",
    "# Save the data to a CSV file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = yfinance.download(tickers=nyse_tickers, start=\"2000-01-01\", end=\"2023-12-31\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('stock_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'AA',\n",
       " 'AA$B',\n",
       " 'AAC',\n",
       " 'AAN',\n",
       " 'AAP',\n",
       " 'AAT',\n",
       " 'AAV',\n",
       " 'AB',\n",
       " 'ABB',\n",
       " 'ABBV',\n",
       " 'ABC',\n",
       " 'ABEV',\n",
       " 'ABG',\n",
       " 'ABM',\n",
       " 'ABR',\n",
       " 'ABR$A',\n",
       " 'ABR$B',\n",
       " 'ABR$C',\n",
       " 'ABRN',\n",
       " 'ABT',\n",
       " 'ABX',\n",
       " 'ACC',\n",
       " 'ACCO',\n",
       " 'ACE',\n",
       " 'ACG',\n",
       " 'ACH',\n",
       " 'ACI',\n",
       " 'ACM',\n",
       " 'ACMP',\n",
       " 'ACN',\n",
       " 'ACP',\n",
       " 'ACRE',\n",
       " 'ACT',\n",
       " 'ACW',\n",
       " 'ADC',\n",
       " 'ADM',\n",
       " 'ADPT',\n",
       " 'ADS',\n",
       " 'ADT',\n",
       " 'ADX',\n",
       " 'AEB',\n",
       " 'AEC',\n",
       " 'AED',\n",
       " 'AEE',\n",
       " 'AEG',\n",
       " 'AEH',\n",
       " 'AEK',\n",
       " 'AEL',\n",
       " 'AEM',\n",
       " 'AEO',\n",
       " 'AEP',\n",
       " 'AER',\n",
       " 'AES',\n",
       " 'AES$C',\n",
       " 'AET',\n",
       " 'AF',\n",
       " 'AF$C',\n",
       " 'AFA',\n",
       " 'AFB',\n",
       " 'AFC',\n",
       " 'AFG',\n",
       " 'AFGE',\n",
       " 'AFL',\n",
       " 'AFM',\n",
       " 'AFQ',\n",
       " 'AFSD',\n",
       " 'AFSI$A',\n",
       " 'AFSI$B',\n",
       " 'AFSI$C',\n",
       " 'AFT',\n",
       " 'AFW',\n",
       " 'AG',\n",
       " 'AGC',\n",
       " 'AGCO',\n",
       " 'AGD',\n",
       " 'AGI',\n",
       " 'AGM',\n",
       " 'AGM$A',\n",
       " 'AGM$B',\n",
       " 'AGM$C',\n",
       " 'AGM.A',\n",
       " 'AGN',\n",
       " 'AGO',\n",
       " 'AGO$B',\n",
       " 'AGO$E',\n",
       " 'AGO$F',\n",
       " 'AGRO',\n",
       " 'AGU',\n",
       " 'AGX',\n",
       " 'AHC',\n",
       " 'AHH',\n",
       " 'AHL',\n",
       " 'AHL$A',\n",
       " 'AHL$B',\n",
       " 'AHL$C',\n",
       " 'AHP',\n",
       " 'AHS',\n",
       " 'AHT',\n",
       " 'AHT$A',\n",
       " 'AHT$D',\n",
       " 'AHT$E',\n",
       " 'AI',\n",
       " 'AIB',\n",
       " 'AIF',\n",
       " 'AIG',\n",
       " 'AIG.W',\n",
       " 'AIN',\n",
       " 'AIR',\n",
       " 'AIT',\n",
       " 'AIV',\n",
       " 'AIV$A',\n",
       " 'AIV$Z',\n",
       " 'AIW',\n",
       " 'AIY',\n",
       " 'AIZ',\n",
       " 'AJG',\n",
       " 'AKO.A',\n",
       " 'AKO.B',\n",
       " 'AKP',\n",
       " 'AKR',\n",
       " 'AKS',\n",
       " 'AL',\n",
       " 'ALB',\n",
       " 'ALDW',\n",
       " 'ALE',\n",
       " 'ALEX',\n",
       " 'ALG',\n",
       " 'ALJ',\n",
       " 'ALK',\n",
       " 'ALL',\n",
       " 'ALL$A',\n",
       " 'ALL$B',\n",
       " 'ALL$C',\n",
       " 'ALL$D',\n",
       " 'ALL$E',\n",
       " 'ALL$F',\n",
       " 'ALLE',\n",
       " 'ALLY',\n",
       " 'ALLY$A',\n",
       " 'ALLY$B',\n",
       " 'ALP$N',\n",
       " 'ALP$O',\n",
       " 'ALP$P',\n",
       " 'ALR',\n",
       " 'ALR$B',\n",
       " 'ALSN',\n",
       " 'ALU',\n",
       " 'ALV',\n",
       " 'ALX',\n",
       " 'AM',\n",
       " 'AMBR',\n",
       " 'AMC',\n",
       " 'AME',\n",
       " 'AMFW',\n",
       " 'AMG',\n",
       " 'AMH',\n",
       " 'AMH$A',\n",
       " 'AMH$B',\n",
       " 'AMH$C',\n",
       " 'AMID',\n",
       " 'AMP',\n",
       " 'AMRC',\n",
       " 'AMRE',\n",
       " 'AMT',\n",
       " 'AMT$A',\n",
       " 'AMTD',\n",
       " 'AMTG',\n",
       " 'AMTG$A',\n",
       " 'AMX',\n",
       " 'AN',\n",
       " 'ANET',\n",
       " 'ANF',\n",
       " 'ANFI',\n",
       " 'ANH',\n",
       " 'ANH$A',\n",
       " 'ANH$B',\n",
       " 'ANN',\n",
       " 'ANR',\n",
       " 'ANTM',\n",
       " 'ANW',\n",
       " 'AOD',\n",
       " 'AOI',\n",
       " 'AOL',\n",
       " 'AON',\n",
       " 'AOS',\n",
       " 'AP',\n",
       " 'APA',\n",
       " 'APAM',\n",
       " 'APB',\n",
       " 'APC',\n",
       " 'APD',\n",
       " 'APF',\n",
       " 'APH',\n",
       " 'APL',\n",
       " 'APL$E',\n",
       " 'APO',\n",
       " 'APU',\n",
       " 'AR',\n",
       " 'ARC',\n",
       " 'ARCO',\n",
       " 'ARCX',\n",
       " 'ARDC',\n",
       " 'ARE',\n",
       " 'ARE$E',\n",
       " 'ARES',\n",
       " 'ARG',\n",
       " 'ARH$C',\n",
       " 'ARI',\n",
       " 'ARI$A',\n",
       " 'ARL',\n",
       " 'ARMF',\n",
       " 'ARMK',\n",
       " 'ARN',\n",
       " 'ARO',\n",
       " 'ARP',\n",
       " 'ARP$D',\n",
       " 'ARPI',\n",
       " 'ARR',\n",
       " 'ARR$A',\n",
       " 'ARR$B',\n",
       " 'ARU',\n",
       " 'ARW',\n",
       " 'ARY',\n",
       " 'ASA',\n",
       " 'ASB',\n",
       " 'ASB$B',\n",
       " 'ASC',\n",
       " 'ASG',\n",
       " 'ASGN',\n",
       " 'ASH',\n",
       " 'ASPN',\n",
       " 'ASR',\n",
       " 'ASX',\n",
       " 'AT',\n",
       " 'ATE',\n",
       " 'ATEN',\n",
       " 'ATHM',\n",
       " 'ATI',\n",
       " 'ATK',\n",
       " 'ATLS',\n",
       " 'ATO',\n",
       " 'ATR',\n",
       " 'ATTO',\n",
       " 'ATU',\n",
       " 'ATV',\n",
       " 'ATW',\n",
       " 'AU',\n",
       " 'AUO',\n",
       " 'AUQ',\n",
       " 'AUY',\n",
       " 'AV',\n",
       " 'AVA',\n",
       " 'AVAL',\n",
       " 'AVB',\n",
       " 'AVD',\n",
       " 'AVG',\n",
       " 'AVH',\n",
       " 'AVIV',\n",
       " 'AVK',\n",
       " 'AVOL',\n",
       " 'AVP',\n",
       " 'AVT',\n",
       " 'AVV',\n",
       " 'AVX',\n",
       " 'AVY',\n",
       " 'AWF',\n",
       " 'AWH',\n",
       " 'AWI',\n",
       " 'AWK',\n",
       " 'AWP',\n",
       " 'AWR',\n",
       " 'AXE',\n",
       " 'AXL',\n",
       " 'AXLL',\n",
       " 'AXP',\n",
       " 'AXR',\n",
       " 'AXS',\n",
       " 'AXS$C',\n",
       " 'AXS$D',\n",
       " 'AXTA',\n",
       " 'AYI',\n",
       " 'AYN',\n",
       " 'AYR',\n",
       " 'AZN',\n",
       " 'AZO',\n",
       " 'AZZ',\n",
       " 'B',\n",
       " 'BA',\n",
       " 'BABA',\n",
       " 'BAC',\n",
       " 'BAC$D',\n",
       " 'BAC$E',\n",
       " 'BAC$I',\n",
       " 'BAC$L',\n",
       " 'BAC$W',\n",
       " 'BAC$Z',\n",
       " 'BAC.A',\n",
       " 'BAC.B',\n",
       " 'BAF',\n",
       " 'BAH',\n",
       " 'BAK',\n",
       " 'BALT',\n",
       " 'BAM',\n",
       " 'BANC',\n",
       " 'BANC$C',\n",
       " 'BAP',\n",
       " 'BAS',\n",
       " 'BAX',\n",
       " 'BBD',\n",
       " 'BBDO',\n",
       " 'BBF',\n",
       " 'BBG',\n",
       " 'BBK',\n",
       " 'BBL',\n",
       " 'BBN',\n",
       " 'BBT',\n",
       " 'BBT$D',\n",
       " 'BBT$E',\n",
       " 'BBT$F',\n",
       " 'BBT$G',\n",
       " 'BBVA',\n",
       " 'BBW',\n",
       " 'BBX',\n",
       " 'BBY',\n",
       " 'BC',\n",
       " 'BCA',\n",
       " 'BCC',\n",
       " 'BCE',\n",
       " 'BCEI',\n",
       " 'BCH',\n",
       " 'BCO',\n",
       " 'BCR',\n",
       " 'BCRH',\n",
       " 'BCS',\n",
       " 'BCS$',\n",
       " 'BCS$A',\n",
       " 'BCS$C',\n",
       " 'BCS$D',\n",
       " 'BCX',\n",
       " 'BDC',\n",
       " 'BDJ',\n",
       " 'BDN',\n",
       " 'BDN$E',\n",
       " 'BDX',\n",
       " 'BEE',\n",
       " 'BEL',\n",
       " 'BEN',\n",
       " 'BEP',\n",
       " 'BERY',\n",
       " 'BF.A',\n",
       " 'BF.B',\n",
       " 'BFAM',\n",
       " 'BFK',\n",
       " 'BFO',\n",
       " 'BFR',\n",
       " 'BFS',\n",
       " 'BFS$C',\n",
       " 'BFZ',\n",
       " 'BG',\n",
       " 'BGB',\n",
       " 'BGC',\n",
       " 'BGCA',\n",
       " 'BGE$B',\n",
       " 'BGG',\n",
       " 'BGH',\n",
       " 'BGR',\n",
       " 'BGS',\n",
       " 'BGT',\n",
       " 'BGX',\n",
       " 'BGY',\n",
       " 'BH',\n",
       " 'BHE',\n",
       " 'BHI',\n",
       " 'BHK',\n",
       " 'BHL',\n",
       " 'BHLB',\n",
       " 'BHP',\n",
       " 'BID',\n",
       " 'BIE',\n",
       " 'BIF',\n",
       " 'BIG',\n",
       " 'BIN',\n",
       " 'BIO',\n",
       " 'BIO.B',\n",
       " 'BIOA',\n",
       " 'BIOA.W',\n",
       " 'BIP',\n",
       " 'BIT',\n",
       " 'BITA',\n",
       " 'BJZ',\n",
       " 'BK',\n",
       " 'BK$C',\n",
       " 'BKD',\n",
       " 'BKE',\n",
       " 'BKH',\n",
       " 'BKK',\n",
       " 'BKN',\n",
       " 'BKS',\n",
       " 'BKT',\n",
       " 'BKU',\n",
       " 'BLH',\n",
       " 'BLK',\n",
       " 'BLL',\n",
       " 'BLOX',\n",
       " 'BLT',\n",
       " 'BLW',\n",
       " 'BLX',\n",
       " 'BMA',\n",
       " 'BME',\n",
       " 'BMI',\n",
       " 'BML$G',\n",
       " 'BML$H',\n",
       " 'BML$I',\n",
       " 'BML$J',\n",
       " 'BML$L',\n",
       " 'BMO',\n",
       " 'BMR',\n",
       " 'BMS',\n",
       " 'BMY',\n",
       " 'BNJ',\n",
       " 'BNK',\n",
       " 'BNS',\n",
       " 'BNY',\n",
       " 'BOCA',\n",
       " 'BOE',\n",
       " 'BOH',\n",
       " 'BOI',\n",
       " 'BOOT',\n",
       " 'BORN',\n",
       " 'BOXC',\n",
       " 'BP',\n",
       " 'BPI',\n",
       " 'BPK',\n",
       " 'BPL',\n",
       " 'BPT',\n",
       " 'BPY',\n",
       " 'BPZ',\n",
       " 'BQH',\n",
       " 'BR',\n",
       " 'BRC',\n",
       " 'BRFS',\n",
       " 'BRK.A',\n",
       " 'BRK.B',\n",
       " 'BRO',\n",
       " 'BRP',\n",
       " 'BRS',\n",
       " 'BRSS',\n",
       " 'BRT',\n",
       " 'BRX',\n",
       " 'BSAC',\n",
       " 'BSBR',\n",
       " 'BSD',\n",
       " 'BSE',\n",
       " 'BSI',\n",
       " 'BSL',\n",
       " 'BSMX',\n",
       " 'BST',\n",
       " 'BSX',\n",
       " 'BT',\n",
       " 'BTA',\n",
       " 'BTE',\n",
       " 'BTF',\n",
       " 'BTH',\n",
       " 'BTO',\n",
       " 'BTT',\n",
       " 'BTU',\n",
       " 'BTZ',\n",
       " 'BUD',\n",
       " 'BUI',\n",
       " 'BURL',\n",
       " 'BVN',\n",
       " 'BWA',\n",
       " 'BWC',\n",
       " 'BWG',\n",
       " 'BWP',\n",
       " 'BWS',\n",
       " 'BX',\n",
       " 'BXC',\n",
       " 'BXE',\n",
       " 'BXMT',\n",
       " 'BXMX',\n",
       " 'BXP',\n",
       " 'BXP$B',\n",
       " 'BXS',\n",
       " 'BYD',\n",
       " 'BYM',\n",
       " 'BZH',\n",
       " 'BZT',\n",
       " 'C',\n",
       " 'C$C',\n",
       " 'C$J',\n",
       " 'C$K',\n",
       " 'C$L',\n",
       " 'C$N',\n",
       " 'C$P',\n",
       " 'C.A',\n",
       " 'C.B',\n",
       " 'CAB',\n",
       " 'CACI',\n",
       " 'CAE',\n",
       " 'CAF',\n",
       " 'CAG',\n",
       " 'CAH',\n",
       " 'CAJ',\n",
       " 'CALX',\n",
       " 'CAM',\n",
       " 'CAP',\n",
       " 'CAPL',\n",
       " 'CAS',\n",
       " 'CAT',\n",
       " 'CATO',\n",
       " 'CB',\n",
       " 'CBA',\n",
       " 'CBB',\n",
       " 'CBB$B',\n",
       " 'CBD',\n",
       " 'CBG',\n",
       " 'CBI',\n",
       " 'CBK',\n",
       " 'CBL',\n",
       " 'CBL$D',\n",
       " 'CBL$E',\n",
       " 'CBM',\n",
       " 'CBPX',\n",
       " 'CBR',\n",
       " 'CBS',\n",
       " 'CBS.A',\n",
       " 'CBT',\n",
       " 'CBU',\n",
       " 'CBZ',\n",
       " 'CCC',\n",
       " 'CCE',\n",
       " 'CCG',\n",
       " 'CCG$A',\n",
       " 'CCI',\n",
       " 'CCI$A',\n",
       " 'CCJ',\n",
       " 'CCK',\n",
       " 'CCL',\n",
       " 'CCM',\n",
       " 'CCO',\n",
       " 'CCS',\n",
       " 'CCSC',\n",
       " 'CCU',\n",
       " 'CCV',\n",
       " 'CCZ',\n",
       " 'CDE',\n",
       " 'CDE.W',\n",
       " 'CDI',\n",
       " 'CDR',\n",
       " 'CDR$B',\n",
       " 'CE',\n",
       " 'CEA',\n",
       " 'CEB',\n",
       " 'CEE',\n",
       " 'CEL',\n",
       " 'CELP',\n",
       " 'CEM',\n",
       " 'CEN',\n",
       " 'CEO',\n",
       " 'CEQP',\n",
       " 'CF',\n",
       " 'CFC$A',\n",
       " 'CFC$B',\n",
       " 'CFG',\n",
       " 'CFI',\n",
       " 'CFN',\n",
       " 'CFR',\n",
       " 'CFR$A',\n",
       " 'CFX',\n",
       " 'CGA',\n",
       " 'CGG',\n",
       " 'CGI',\n",
       " 'CHA',\n",
       " 'CHD',\n",
       " 'CHE',\n",
       " 'CHGG',\n",
       " 'CHH',\n",
       " 'CHK',\n",
       " 'CHK$D',\n",
       " 'CHKR',\n",
       " 'CHL',\n",
       " 'CHMI',\n",
       " 'CHMT',\n",
       " 'CHN',\n",
       " 'CHS',\n",
       " 'CHSP',\n",
       " 'CHSP$A',\n",
       " 'CHT',\n",
       " 'CHU',\n",
       " 'CI',\n",
       " 'CIA',\n",
       " 'CIB',\n",
       " 'CIE',\n",
       " 'CIEN',\n",
       " 'CIF',\n",
       " 'CIG',\n",
       " 'CIG.C',\n",
       " 'CII',\n",
       " 'CIM',\n",
       " 'CIO',\n",
       " 'CIR',\n",
       " 'CIT',\n",
       " 'CIVI',\n",
       " 'CJES',\n",
       " 'CKH',\n",
       " 'CKP',\n",
       " 'CL',\n",
       " 'CLA',\n",
       " 'CLB',\n",
       " 'CLC',\n",
       " 'CLD',\n",
       " 'CLDT',\n",
       " 'CLF',\n",
       " 'CLGX',\n",
       " 'CLH',\n",
       " 'CLI',\n",
       " 'CLNY',\n",
       " 'CLNY$A',\n",
       " 'CLNY$B',\n",
       " 'CLR',\n",
       " 'CLS',\n",
       " 'CLV',\n",
       " 'CLW',\n",
       " 'CLX',\n",
       " 'CM',\n",
       " 'CMA',\n",
       " 'CMA.W',\n",
       " 'CMC',\n",
       " 'CMCM',\n",
       " 'CMG',\n",
       " 'CMI',\n",
       " 'CMK',\n",
       " 'CMLP',\n",
       " 'CMN',\n",
       " 'CMO',\n",
       " 'CMO$E',\n",
       " 'CMP',\n",
       " 'CMRE',\n",
       " 'CMRE$B',\n",
       " 'CMRE$C',\n",
       " 'CMS',\n",
       " 'CMS$B',\n",
       " 'CMU',\n",
       " 'CNA',\n",
       " 'CNC',\n",
       " 'CNCO',\n",
       " 'CNHI',\n",
       " 'CNI',\n",
       " 'CNK',\n",
       " 'CNL',\n",
       " 'CNNX',\n",
       " 'CNO',\n",
       " 'CNP',\n",
       " 'CNQ',\n",
       " 'CNS',\n",
       " 'CNW',\n",
       " 'CNX',\n",
       " 'CO',\n",
       " 'CODE',\n",
       " 'CODI',\n",
       " 'COF',\n",
       " 'COF$C',\n",
       " 'COF$D',\n",
       " 'COF$P',\n",
       " 'COF.W',\n",
       " 'COG',\n",
       " 'COH',\n",
       " 'COL',\n",
       " 'COO',\n",
       " 'COP',\n",
       " 'COR',\n",
       " 'COR$A',\n",
       " 'CORR',\n",
       " 'COT',\n",
       " 'COTY',\n",
       " 'COUP',\n",
       " 'COV',\n",
       " 'CP',\n",
       " 'CPA',\n",
       " 'CPAC',\n",
       " 'CPB',\n",
       " 'CPE',\n",
       " 'CPE$A',\n",
       " 'CPF',\n",
       " 'CPG',\n",
       " 'CPK',\n",
       " 'CPL',\n",
       " 'CPN',\n",
       " 'CPS',\n",
       " 'CPT',\n",
       " 'CR',\n",
       " 'CRC',\n",
       " 'CRCM',\n",
       " 'CRD.A',\n",
       " 'CRD.B',\n",
       " 'CRH',\n",
       " 'CRI',\n",
       " 'CRK',\n",
       " 'CRL',\n",
       " 'CRM',\n",
       " 'CRR',\n",
       " 'CRS',\n",
       " 'CRT',\n",
       " 'CRY',\n",
       " 'CS',\n",
       " 'CSC',\n",
       " 'CSG',\n",
       " 'CSH',\n",
       " 'CSI',\n",
       " 'CSL',\n",
       " 'CSLT',\n",
       " 'CSS',\n",
       " 'CST',\n",
       " 'CSTM',\n",
       " 'CSU',\n",
       " 'CSV',\n",
       " 'CSX',\n",
       " 'CTB',\n",
       " 'CTL',\n",
       " 'CTLT',\n",
       " 'CTQ',\n",
       " 'CTR',\n",
       " 'CTS',\n",
       " 'CTT',\n",
       " 'CTU',\n",
       " 'CTV',\n",
       " 'CTW',\n",
       " 'CTX',\n",
       " 'CTY',\n",
       " 'CUB',\n",
       " 'CUBE',\n",
       " 'CUBE$A',\n",
       " 'CUBI',\n",
       " 'CUBS',\n",
       " 'CUDA',\n",
       " 'CUK',\n",
       " 'CUZ',\n",
       " 'CVA',\n",
       " 'CVB',\n",
       " 'CVC',\n",
       " 'CVD',\n",
       " 'CVE',\n",
       " 'CVEO',\n",
       " 'CVG',\n",
       " 'CVI',\n",
       " 'CVO',\n",
       " 'CVRR',\n",
       " 'CVS',\n",
       " 'CVT',\n",
       " 'CVX',\n",
       " 'CW',\n",
       " 'CWEI',\n",
       " 'CWT',\n",
       " 'CX',\n",
       " 'CXE',\n",
       " 'CXH',\n",
       " 'CXO',\n",
       " 'CXP',\n",
       " 'CXW',\n",
       " 'CYD',\n",
       " 'CYH',\n",
       " 'CYN',\n",
       " 'CYN$C',\n",
       " 'CYN$D',\n",
       " 'CYNI',\n",
       " 'CYS',\n",
       " 'CYS$A',\n",
       " 'CYS$B',\n",
       " 'CYT',\n",
       " 'CZZ',\n",
       " 'D',\n",
       " 'DAC',\n",
       " 'DAL',\n",
       " 'DAN',\n",
       " 'DANG',\n",
       " 'DAR',\n",
       " 'DATA',\n",
       " 'DB',\n",
       " 'DBD',\n",
       " 'DBL',\n",
       " 'DCA',\n",
       " 'DCI',\n",
       " 'DCM',\n",
       " 'DCO',\n",
       " 'DCT',\n",
       " 'DCUA',\n",
       " 'DCUB',\n",
       " 'DCUC',\n",
       " 'DD',\n",
       " 'DD$A',\n",
       " 'DD$B',\n",
       " 'DDC',\n",
       " 'DDD',\n",
       " 'DDE',\n",
       " 'DDF',\n",
       " 'DDR',\n",
       " 'DDR$J',\n",
       " 'DDR$K',\n",
       " 'DDS',\n",
       " 'DDT',\n",
       " 'DE',\n",
       " 'DECK',\n",
       " 'DEG',\n",
       " 'DEI',\n",
       " 'DEL',\n",
       " 'DEO',\n",
       " 'DEX',\n",
       " 'DF',\n",
       " 'DFP',\n",
       " 'DFS',\n",
       " 'DFS$B',\n",
       " 'DFT',\n",
       " 'DFT$A',\n",
       " 'DFT$B',\n",
       " 'DG',\n",
       " 'DGI',\n",
       " 'DGX',\n",
       " 'DHF',\n",
       " 'DHG',\n",
       " 'DHI',\n",
       " 'DHR',\n",
       " 'DHT',\n",
       " 'DHX',\n",
       " 'DIAX',\n",
       " 'DIN',\n",
       " 'DIS',\n",
       " 'DK',\n",
       " 'DKL',\n",
       " 'DKS',\n",
       " 'DKT',\n",
       " 'DL',\n",
       " 'DLB',\n",
       " 'DLNG',\n",
       " 'DLPH',\n",
       " 'DLR',\n",
       " 'DLR$E',\n",
       " 'DLR$F',\n",
       " 'DLR$G',\n",
       " 'DLR$H',\n",
       " 'DLX',\n",
       " 'DM',\n",
       " 'DMB',\n",
       " 'DMD',\n",
       " 'DMO',\n",
       " 'DNB',\n",
       " 'DNI',\n",
       " 'DNOW',\n",
       " 'DNP',\n",
       " 'DNR',\n",
       " 'DNY',\n",
       " 'DO',\n",
       " 'DOC',\n",
       " 'DOM',\n",
       " 'DOOR',\n",
       " 'DOV',\n",
       " 'DOW',\n",
       " 'DPG',\n",
       " 'DPLO',\n",
       " 'DPM',\n",
       " 'DPS',\n",
       " 'DPZ',\n",
       " 'DQ',\n",
       " 'DRA',\n",
       " 'DRC',\n",
       " 'DRD',\n",
       " 'DRE',\n",
       " 'DRH',\n",
       " 'DRI',\n",
       " 'DRII',\n",
       " 'DRL',\n",
       " 'DRQ',\n",
       " 'DSE',\n",
       " 'DSL',\n",
       " 'DSM',\n",
       " 'DST',\n",
       " 'DSU',\n",
       " 'DSW',\n",
       " 'DSX',\n",
       " 'DSX$B',\n",
       " 'DTE',\n",
       " 'DTF',\n",
       " 'DTK',\n",
       " 'DTLA$',\n",
       " 'DTQ',\n",
       " 'DTT',\n",
       " 'DTZ',\n",
       " 'DUA',\n",
       " 'DUC',\n",
       " 'DUK',\n",
       " 'DUKH',\n",
       " 'DV',\n",
       " 'DVA',\n",
       " 'DVD',\n",
       " 'DVN',\n",
       " 'DW',\n",
       " 'DWRE',\n",
       " 'DX',\n",
       " 'DX$A',\n",
       " 'DX$B',\n",
       " 'DXB',\n",
       " 'DY',\n",
       " 'DYN',\n",
       " 'DYN$A',\n",
       " 'DYN.W',\n",
       " 'E',\n",
       " 'EAA',\n",
       " 'EAB',\n",
       " 'EAE',\n",
       " 'EARN',\n",
       " 'EAT',\n",
       " 'EBF',\n",
       " 'EBR',\n",
       " 'EBR.B',\n",
       " 'EBS',\n",
       " 'EC',\n",
       " 'ECA',\n",
       " 'ECC',\n",
       " 'ECL',\n",
       " 'ECOM',\n",
       " 'ECR',\n",
       " 'ECT',\n",
       " 'ED',\n",
       " 'EDD',\n",
       " 'EDE',\n",
       " 'EDF',\n",
       " 'EDI',\n",
       " 'EDN',\n",
       " 'EDR',\n",
       " 'EDU',\n",
       " 'EE',\n",
       " 'EEA',\n",
       " 'EEP',\n",
       " 'EEQ',\n",
       " 'EFC',\n",
       " 'EFF',\n",
       " 'EFM',\n",
       " 'EFR',\n",
       " 'EFT',\n",
       " 'EFX',\n",
       " 'EGF',\n",
       " 'EGL',\n",
       " 'EGN',\n",
       " 'EGO',\n",
       " 'EGP',\n",
       " 'EGY',\n",
       " 'EHI',\n",
       " 'EHIC',\n",
       " 'EIG',\n",
       " 'EIX',\n",
       " 'EJ',\n",
       " 'EL',\n",
       " 'ELA',\n",
       " 'ELB',\n",
       " 'ELJ',\n",
       " 'ELLI',\n",
       " 'ELP',\n",
       " 'ELS',\n",
       " 'ELS$C',\n",
       " 'ELU',\n",
       " 'ELX',\n",
       " 'ELY',\n",
       " 'EMC',\n",
       " 'EMD',\n",
       " 'EME',\n",
       " 'EMES',\n",
       " 'EMF',\n",
       " 'EMN',\n",
       " 'EMO',\n",
       " 'EMQ',\n",
       " 'EMR',\n",
       " 'EMZ',\n",
       " 'ENB',\n",
       " 'ENBL',\n",
       " 'ENH',\n",
       " 'ENH$A',\n",
       " 'ENH$B',\n",
       " 'ENI',\n",
       " 'ENJ',\n",
       " 'ENL',\n",
       " 'ENLC',\n",
       " 'ENLK',\n",
       " 'ENR',\n",
       " 'ENS',\n",
       " 'ENV',\n",
       " 'ENVA',\n",
       " 'ENZ',\n",
       " 'EOC',\n",
       " 'EOD',\n",
       " 'EOG',\n",
       " 'EOI',\n",
       " 'EOS',\n",
       " 'EOT',\n",
       " 'EP$C',\n",
       " 'EPAM',\n",
       " 'EPD',\n",
       " 'EPE',\n",
       " 'EPR',\n",
       " 'EPR$C',\n",
       " ...]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tickers = list(map(lambda x: x[1], data.columns))\n",
    "tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the date range\n",
    "start_date = pd.to_datetime('2000-01-01')\n",
    "end_date = pd.to_datetime('2021-01-10')\n",
    "\n",
    "# Initialize an empty DataFrame to store the data\n",
    "adbe_data = pd.DataFrame()\n",
    "\n",
    "# Loop to fetch data in 7-day intervals\n",
    "current_start = start_date\n",
    "while current_start < end_date:\n",
    "    current_end = current_start + pd.Timedelta(days=7)\n",
    "    if current_end > end_date:\n",
    "        current_end = end_date\n",
    "    temp_data = yf.download('ADBE', interval='1h', start=current_start.strftime('%Y-%m-%d'), end=current_end.strftime('%Y-%m-%d'))\n",
    "    adbe_data = pd.concat([adbe_data, temp_data])\n",
    "    current_start = current_end\n",
    "\n",
    "adbe = adbe_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "adbe = yf.download('ADBE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-08-13</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.198057</td>\n",
       "      <td>18899200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-08-14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230469</td>\n",
       "      <td>0.222656</td>\n",
       "      <td>0.222656</td>\n",
       "      <td>0.209060</td>\n",
       "      <td>4160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-08-15</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222656</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.205392</td>\n",
       "      <td>4332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-08-18</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.210938</td>\n",
       "      <td>0.198057</td>\n",
       "      <td>2828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-08-19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218750</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.201725</td>\n",
       "      <td>2060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-07</th>\n",
       "      <td>458.130005</td>\n",
       "      <td>468.929993</td>\n",
       "      <td>458.130005</td>\n",
       "      <td>465.429993</td>\n",
       "      <td>465.429993</td>\n",
       "      <td>2684200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-10</th>\n",
       "      <td>453.839996</td>\n",
       "      <td>462.230011</td>\n",
       "      <td>451.510010</td>\n",
       "      <td>459.940002</td>\n",
       "      <td>459.940002</td>\n",
       "      <td>3212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-11</th>\n",
       "      <td>456.910004</td>\n",
       "      <td>462.739990</td>\n",
       "      <td>455.500000</td>\n",
       "      <td>462.690002</td>\n",
       "      <td>462.690002</td>\n",
       "      <td>2723700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-12</th>\n",
       "      <td>470.000000</td>\n",
       "      <td>471.119995</td>\n",
       "      <td>459.160004</td>\n",
       "      <td>459.869995</td>\n",
       "      <td>459.869995</td>\n",
       "      <td>3647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-06-13</th>\n",
       "      <td>457.869995</td>\n",
       "      <td>462.390015</td>\n",
       "      <td>453.500000</td>\n",
       "      <td>458.739990</td>\n",
       "      <td>458.739990</td>\n",
       "      <td>7628000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9535 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "1986-08-13    0.000000    0.218750    0.210938    0.210938    0.198057   \n",
       "1986-08-14    0.000000    0.230469    0.222656    0.222656    0.209060   \n",
       "1986-08-15    0.000000    0.222656    0.218750    0.218750    0.205392   \n",
       "1986-08-18    0.000000    0.218750    0.210938    0.210938    0.198057   \n",
       "1986-08-19    0.000000    0.218750    0.214844    0.214844    0.201725   \n",
       "...                ...         ...         ...         ...         ...   \n",
       "2024-06-07  458.130005  468.929993  458.130005  465.429993  465.429993   \n",
       "2024-06-10  453.839996  462.230011  451.510010  459.940002  459.940002   \n",
       "2024-06-11  456.910004  462.739990  455.500000  462.690002  462.690002   \n",
       "2024-06-12  470.000000  471.119995  459.160004  459.869995  459.869995   \n",
       "2024-06-13  457.869995  462.390015  453.500000  458.739990  458.739990   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "1986-08-13  18899200  \n",
       "1986-08-14   4160000  \n",
       "1986-08-15   4332800  \n",
       "1986-08-18   2828800  \n",
       "1986-08-19   2060800  \n",
       "...              ...  \n",
       "2024-06-07   2684200  \n",
       "2024-06-10   3212400  \n",
       "2024-06-11   2723700  \n",
       "2024-06-12   3647400  \n",
       "2024-06-13   7628000  \n",
       "\n",
       "[9535 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adbe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "close = np.array(adbe['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Date'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "adbe['Close'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('adbe.txt', close, fmt='%.2f', newline=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98901"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "999*99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 99900 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(GPTConfig())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(98304, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=98304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[21, 22, 22, 21, 21, 21, 22, 22, 21, 21, 21, 21, 21, 22, 22, 25, 25, 24,\n",
       "         25, 26, 23, 23, 26, 27, 28, 28, 28, 28, 27, 27, 27, 27],\n",
       "        [27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 28, 28, 28, 28, 28, 28, 28, 28,\n",
       "         28, 32, 32, 32, 35, 37, 37, 35, 34, 34, 35, 35, 36, 36],\n",
       "        [36, 36, 36, 36, 35, 35, 35, 35, 35, 35, 37, 38, 38, 39, 38, 39, 39, 41,\n",
       "         40, 40, 37, 38, 37, 37, 36, 37, 37, 37, 38, 39, 40, 39],\n",
       "        [39, 40, 41, 42, 42, 43, 44, 44, 46, 47, 48, 48, 49, 51, 54, 54, 53, 56,\n",
       "         55, 56, 56, 54, 53, 54, 55, 56, 57, 57, 57, 59, 63, 67]],\n",
       "       device='mps:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokens = np.loadtxt('adbe.txt')\n",
    "tokens = tokens*100\n",
    "tokens = tokens.astype(np.int32)\n",
    "B, T = 4,1024\n",
    "\n",
    "\n",
    "buf  = torch.tensor(tokens[:B*T+1])\n",
    "buf = buf.to('mps')\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 11.080656051635742\n",
      "Step 1, loss: 8.474421501159668\n",
      "Step 2, loss: 10.110151290893555\n",
      "Step 3, loss: 6.6910271644592285\n",
      "Step 4, loss: 5.74012565612793\n",
      "Step 5, loss: 5.113059997558594\n",
      "Step 6, loss: 4.32537317276001\n",
      "Step 7, loss: 3.821817636489868\n",
      "Step 8, loss: 3.302058219909668\n",
      "Step 9, loss: 2.896960735321045\n",
      "Step 10, loss: 2.6457273960113525\n",
      "Step 11, loss: 2.469611644744873\n",
      "Step 12, loss: 2.327191114425659\n",
      "Step 13, loss: 2.2289278507232666\n",
      "Step 14, loss: 2.184169292449951\n",
      "Step 15, loss: 2.1077072620391846\n",
      "Step 16, loss: 2.1641669273376465\n",
      "Step 17, loss: 2.0433740615844727\n",
      "Step 18, loss: 2.0060205459594727\n",
      "Step 19, loss: 1.9782664775848389\n",
      "Step 20, loss: 1.9564121961593628\n",
      "Step 21, loss: 1.9200743436813354\n",
      "Step 22, loss: 1.8942066431045532\n",
      "Step 23, loss: 1.867746114730835\n",
      "Step 24, loss: 1.8367516994476318\n",
      "Step 25, loss: 1.8053863048553467\n",
      "Step 26, loss: 1.7736964225769043\n",
      "Step 27, loss: 1.7405351400375366\n",
      "Step 28, loss: 1.7089364528656006\n",
      "Step 29, loss: 1.6859766244888306\n",
      "Step 30, loss: 1.6622412204742432\n",
      "Step 31, loss: 1.639296531677246\n",
      "Step 32, loss: 1.689915657043457\n",
      "Step 33, loss: 1.5906740427017212\n",
      "Step 34, loss: 1.5659279823303223\n",
      "Step 35, loss: 1.5424396991729736\n",
      "Step 36, loss: 1.5168094635009766\n",
      "Step 37, loss: 1.4975258111953735\n",
      "Step 38, loss: 1.481896162033081\n",
      "Step 39, loss: 1.462669014930725\n",
      "Step 40, loss: 1.4435091018676758\n",
      "Step 41, loss: 1.4245970249176025\n",
      "Step 42, loss: 1.4013628959655762\n",
      "Step 43, loss: 1.3821693658828735\n",
      "Step 44, loss: 1.361437201499939\n",
      "Step 45, loss: 1.3458960056304932\n",
      "Step 46, loss: 1.3227343559265137\n",
      "Step 47, loss: 1.3006508350372314\n",
      "Step 48, loss: 1.2811293601989746\n",
      "Step 49, loss: 1.259967565536499\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i  in range(50):\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Step {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom data laoder for finance data\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        token = np.loadtxt('adbe.txt')\n",
    "        token = token*100\n",
    "        token = token.astype(np.int32)\n",
    "        self.tokens = torch.tensor(token)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f\"Epochs are {len(self.tokens) // (B * T)}\")\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T \n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + B * T + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 9451 tokens\n",
      "Epochs are 2\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoaderLite(B=4, T=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(99900, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='tanh')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=99900, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 11.573114395141602\n",
      "Step 1, loss: 11.640247344970703\n",
      "Step 2, loss: 11.199441909790039\n",
      "Step 3, loss: 11.123851776123047\n",
      "Step 4, loss: 10.414773941040039\n",
      "Step 5, loss: 10.632307052612305\n",
      "Step 6, loss: 9.87636661529541\n",
      "Step 7, loss: 10.131949424743652\n",
      "Step 8, loss: 9.793098449707031\n",
      "Step 9, loss: 9.665634155273438\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i  in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    x,y = train_loader.next_batch()\n",
    "    x,y = x.to('mps'), y.to('mps')\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Step {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  write code to fetch adobe data\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "# dataloader\n",
    "# custom data laoder for finance data\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        data  = np.load('stock_returns.npy')\n",
    "        data = np.transpose(data)\n",
    "        print(\"Shape of reshaped data: \", data.shape)\n",
    "\n",
    "        data = data.astype(np.int32)\n",
    "        self.tokens = np.array([data[i][j:j+T] for i in range(data.shape[0]) for j in range(0,len(data[i])-T+1,T)])\n",
    "        self.y = np.array([data[i][j+1:j+T+1] for i in range(data.shape[0]) for j in range(0,len(data[i])-T+1,T)])\n",
    "        \n",
    "        self.tokens = torch.tensor(self.tokens)\n",
    "        self.y = torch.tensor(self.y)\n",
    "        print(\"Shape of tokens: \", self.tokens.shape)\n",
    "        # split = int(0.95 * data.shape[0])\n",
    "        # train_data = data[:split]\n",
    "        # val_data = data[split:]\n",
    "        # print shapes of train and val data\n",
    "        # print(\"Shape of train data: \", train_data.shape)\n",
    "        # print(\"Shape of val data: \", val_data.shape)\n",
    "        # self.tokens = mx.array(train_data)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f\"Epochs are {len(self.tokens) // (B * T)}\")\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        x = self.tokens[self.current_position : self.current_position+B]\n",
    "        y  = self.y[self.current_position : self.current_position+B]\n",
    "        self.current_position += B\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + B  + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "\n",
    "# gptconfig\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 99900 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension embedding dimension\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing scheme\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init params\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters (that require grad)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        if master_process:\n",
    "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        if master_process:\n",
    "            print(f\"using fused AdamW: {use_fused}\")\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
    "        return optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped data:  (10488, 6038)\n",
      "Shape of tokens:  torch.Size([492936, 128])\n",
      "loaded 492936 tokens\n",
      "Epochs are 120\n",
      "tensor([[201, 183, 196,  ..., 190, 193, 200],\n",
      "        [195, 198, 193,  ..., 189, 229, 205],\n",
      "        [193, 197, 199,  ..., 190, 199, 190],\n",
      "        ...,\n",
      "        [203, 199, 196,  ..., 202, 203, 200],\n",
      "        [202, 204, 198,  ..., 199, 202, 204],\n",
      "        [202, 200, 200,  ..., 198, 202, 206]], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to('mps')\n",
    "# torch.compile(model=model )\n",
    "\n",
    "train_loader = DataLoaderLite(32, 128)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i  in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    x,y = train_loader.next_batch()\n",
    "    print(x)\n",
    "    x,y = x.to('mps'), y.to('mps')\n",
    "    logits, loss = model(x, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(f\"Step {i}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "import mlx.utils as utils\n",
    "import numpy as np\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyper params\n",
    "# model\n",
    "ctx_len = 128\n",
    "n_emb = 128\n",
    "dropout = 0.1\n",
    "head_size = 128\n",
    "n_heads = 4 \n",
    "n_layers = 3\n",
    "vocab_size = 401\n",
    "\n",
    "# training\n",
    "num_epochs=20\n",
    "batch_size=64\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (6038, 10488)\n",
      "Shape of reshaped data:  (10488, 6038)\n",
      "Shape of train data:  (9963, 6038)\n",
      "Shape of val data:  (525, 6038)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is the code for tokening text input but we aint using text we using finance bois so we dont need this\n",
    "we will write our own data loader\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# ### Tokenization\n",
    "# with open('./input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "# vocab = sorted(list(set(text)))\n",
    "# vocab_size = len(vocab)\n",
    "# itos = {i:c for i,c in enumerate(vocab)} # int to string\n",
    "# stoi = {c:i for i,c in enumerate(vocab)} # string to int\n",
    "# encode = lambda x: [stoi[c] for c in x]\n",
    "# decode = lambda x: ''.join([itos[i] for i in x])\n",
    "# data = encode(text)\n",
    "# split = int(0.9 * len(data))\n",
    "# train_data = data[:split]\n",
    "# val_data = data[split:]\n",
    "\n",
    "\n",
    "\n",
    "data = np.load('stock_returns.npy')\n",
    "print(\"Shape of data: \", data.shape)\n",
    "#  63 Million tokens\n",
    "\n",
    "\n",
    "data = np.transpose(data)\n",
    "print(\"Shape of reshaped data: \", data.shape)\n",
    "\n",
    "split = int(0.95 * data.shape[0])\n",
    "train_data = data[:split]\n",
    "val_data = data[split:]\n",
    "# print shapes of train and val data\n",
    "print(\"Shape of train data: \", train_data.shape)\n",
    "print(\"Shape of val data: \", val_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom data laoder for finance data\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        data  = np.load('stock_returns.npy')\n",
    "        data = np.transpose(data)\n",
    "        print(\"Shape of reshaped data: \", data.shape)\n",
    "\n",
    "        self.data = data\n",
    "        self.tokens = np.array([data[i][j:j+T] for i in range(data.shape[0]) for j in range(0,len(data[i])-T+1,T)])\n",
    "        self.y = np.array([data[i][j+1:j+T+1] for i in range(data.shape[0]) for j in range(0,len(data[i])-T+1,T)])\n",
    "        print(\"Shape of tokens: \", self.tokens.shape)\n",
    "        # split = int(0.95 * data.shape[0])\n",
    "        # train_data = data[:split]\n",
    "        # val_data = data[split:]\n",
    "        # print shapes of train and val data\n",
    "        # print(\"Shape of train data: \", train_data.shape)\n",
    "        # print(\"Shape of val data: \", val_data.shape)\n",
    "        # self.tokens = mx.array(train_data)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f\"Epochs are {len(self.tokens) // (B * T)}\")\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B = self.B\n",
    "        x = self.tokens[self.current_position : self.current_position+B]\n",
    "        y  = self.y[self.current_position : self.current_position+B]\n",
    "        self.current_position += B\n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + B  + 1 > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reshaped data:  (10488, 6038)\n",
      "Shape of tokens:  (492936, 128)\n",
      "loaded 492936 tokens\n",
      "Epochs are 60\n"
     ]
    }
   ],
   "source": [
    "data = DataLoaderLite(batch_size, ctx_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = data.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Definition\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, n_emb)\n",
    "        self.wpe = nn.Embedding(ctx_len, n_emb)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[Block() for _ in range(n_layers)],\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(dims=n_emb)\n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "        self._init_parameters()\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.wte(x)\n",
    "        pos_emb = self.wpe(mx.arange(T))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    def generate(self, max_new_tokens):\n",
    "        ctx = mx.zeros((1, 1), dtype=mx.int32)\n",
    "        for _ in range(max_new_tokens):\n",
    "          logits = self(ctx[:, -ctx_len:])\n",
    "          logits = logits[:, -1, :]\n",
    "          next_tok = mx.random.categorical(logits, num_samples=1)\n",
    "          ctx = mx.concatenate((ctx, next_tok), axis=1)\n",
    "        return ctx\n",
    "    def _init_parameters(self):\n",
    "        normal_init = nn.init.normal(mean=0.0, std=0.02)\n",
    "        residual_init = nn.init.normal(mean=0.0, std=(0.02 / math.sqrt(2 * n_layers)))\n",
    "        new_params = []\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, nn.layers.linear.Linear):\n",
    "                if 'c_proj' in name:\n",
    "                    new_params.append((name + '.weight', residual_init(module.weight)))\n",
    "                else:\n",
    "                    new_params.append((name + '.weight', normal_init(module.weight)))\n",
    "                if 'bias' in module:\n",
    "                    new_params.append((name + '.bias', mx.zeros(module.bias.shape)))\n",
    "            elif isinstance(module, nn.layers.embedding.Embedding):\n",
    "                new_params.append((name + '.weight', normal_init(module.weight)))\n",
    "        self = self.update(utils.tree_unflatten(new_params))\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.k_proj = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.q_proj = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.v_proj = nn.Linear(n_emb, head_size, bias=False)\n",
    "        indices = mx.arange(ctx_len)\n",
    "        mask = indices[:, None] < indices[None]\n",
    "        self._causal_mask = mask * -1e9\n",
    "        self.c_proj = nn.Linear(head_size, n_emb)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        K = self.k_proj(x)\n",
    "        Q = self.q_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        mha_shape = (B, T, n_heads, head_size//n_heads)\n",
    "        K = mx.as_strided(K, (mha_shape)).transpose([0, 2, 1, 3])\n",
    "        Q = mx.as_strided(Q, (mha_shape)).transpose([0, 2, 1, 3])\n",
    "        V = mx.as_strided(V, (mha_shape)).transpose([0, 2, 1, 3])\n",
    "        attn_weights = (Q @ K.transpose([0, 1, 3, 2])) / math.sqrt(Q.shape[-1])\n",
    "        attn_weights = attn_weights + self._causal_mask[:T, :T]\n",
    "        attn_weights = mx.softmax(attn_weights, axis=-1)\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "        o = (attn_weights @ V)\n",
    "        o = o.transpose([0, 2, 1, 3]).reshape((B, T, head_size))\n",
    "        o = self.c_proj(self.resid_dropout(o))\n",
    "        return o\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(n_emb, 4 * n_emb)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * n_emb, n_emb)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def __call__(self, x):\n",
    "        x = self.gelu(self.c_fc(x))\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP()\n",
    "        self.mha = MultiHeadAttention()\n",
    "        self.ln_1 = nn.LayerNorm(dims=n_emb)\n",
    "        self.ln_2 = nn.LayerNorm(dims=n_emb)\n",
    "    def __call__(self, x):\n",
    "        x = x + self.mha(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "### Training\n",
    "def loss_fn(model, x, y):\n",
    "    logits = model(x)\n",
    "    B, T, C = logits.shape\n",
    "    logits = logits.reshape(B*T, C)\n",
    "    y = y.reshape(B*T)\n",
    "    loss = nn.losses.cross_entropy(logits, y, reduction='mean')\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()\n",
    "mx.eval(model.parameters())\n",
    "loss_and_grad = nn.value_and_grad(model, loss_fn)\n",
    "optimizer = optim.AdamW(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoaderLite(batch_size, ctx_len)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train(True)\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    for input, label in get_batches(X_train, y_train, batch_size):\n",
    "        batch_cnt += 1\n",
    "        loss, grads = loss_and_grad(model, input, label)\n",
    "        optimizer.update(model, grads)\n",
    "        running_loss += loss.item()\n",
    "        # compute new parameters and optimizer state\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "    avg_train_loss = running_loss / batch_cnt\n",
    "    model.train(False) # set eval mode\n",
    "    running_loss = 0\n",
    "    batch_cnt = 0\n",
    "    # for input, label in get_batches(X_val, y_val, batch_size):\n",
    "    #     batch_cnt += 1\n",
    "    #     loss = loss_fn(model, input, label)\n",
    "    #     running_loss += loss.item()\n",
    "    # avg_val_loss = running_loss / batch_cnt\n",
    "    print(f\"Epoch {epoch:2} | train = {avg_train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = decode(model.generate(1000)[0].tolist())\n",
    "print(completion)\n",
    "with open('completions.txt', 'w') as f:\n",
    "    f.write(completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
